# 機械学習レポート10点
下記の要件の通り、区分ごとに単元レポートを作成、ご提出ください。
<br> １）各章につき100文字以上で要点をまとめ、実装演習結果、確認テストについての自身の考察等を取り入れたレポートとする。
<br> 単元毎に①〜④（各１点）を組み合わせ科目の基準点を超えるようにする。（②実装は応用数学以外は必須）
<br> ①単元毎の要点まとめ
<br> ②実装演習キャプチャー（応用数学はなし）
<br> ③「確認テスト」など自身の考察結果（応用数学・機械学習はなし）
<br> ④演習問題や参考図書、修了課題など関連記事レポートによる加点
<br> ２）各科目の基準点が足りない場合、実装演習が不足する場合は差し戻しとする。
<br> ※各章は講義動画および講義資料（PDF）でご確認ください。
## ※実装については別ファイル。
## 線形回帰モデル
①要点
<br> 教師あり学習の回帰のうち一番標準的な線形回帰について学んだ。
<br> ある入力（説明変数）から出力（目的変数）を予測する問題で、線形（1次元の場合は直線）で予測する場合に線形回帰という。
<br> 入力が一変数の場合単回帰、二変数以上の場合重回帰と呼ばれている。
<br> 入力とm次元パラメータ（m:説明変数の個数）の線形結合を出力するモデルである。
<br> 既知である目的変数・説明変数に対し、未知の切片・回帰係数を最小二乗法によって算出する。
<br> 以降は、線形回帰モデル以外でもモデル評価にあたっては概ね同様の手順を踏む
<br> モデル評価にあたっては、データを学習データ・検証データに分け、学習データについてモデルに当てはめ、汎化性能を検証データで確認（ホールドアウト法など）といったステップを踏む。
<br> 
<br> ④ステージテスト・黒本について
<br> ステージテストにおいて、以下が出題された。
<br> ・機械学習の種別について、線形回帰が”教師あり学習の回帰手法”であること
<br> ・学習は”最小二乗法”によって行うこと
<br> ・負の相関係数の意味
<br> ・線形回帰の性質(訓練データが多いほど過学習が起きやすいことなど)
<br> 基本的なアルゴリズムがどんな学習か、どんな手法を使用して導出するかは押さえておきたい。
<br> 基本的な相関係数の意味の理解はもちろん、線形回帰がどのような性質を持っているかは復習しておきたい。
<br> （黒本p.105）
<br> 線形回帰については、パラメータについて線形であればよく、入力については非線形であっても良い。
<br> 出力と入力の関係性は非線形でもよく、”出力とパラメータの関係が線形である"ことも覚えておきたい。

## 非線形回帰モデル
①要点
<br> 教師あり学習の回帰のうち、非線形回帰（線形回帰でない回帰）について学んだ。
<br> 線形回帰のように、データの関係性を線形で捉えられることは少ないため、
<br> 構造を非線形で捉えることが必要な場合がある。
<br> また、モデル評価について改めて学習した。既に線形回帰で学んだ、ホールドアウト法は十分なデータ量がないと良い評価を与えないため、
<br> データ量が十分でない時には、クロスバリエーション（交差検証）と呼ばれる、学習用と検証用を交差させる検証法を使用する。
<br> 全てのデータを学習に反映させているため、ホールドアウト法に比べて優れている。

## ロジスティック回帰モデル
①要点
<br> 教師あり学習のうち、線形回帰・非線形回帰とは異なり、目的変数が数値ではなく、クラスに分類する手法
の1つであるロジスティック回帰について学んだ。
<br> 回帰という名前ではあるが、分類を扱うアルゴリズムであることに注意。
<br> m次元の説明変数に対して、目的変数として0 or 1(クラス)をとるデータを考える。
<br> こういったデータを扱うために、ある関数を考える。
<br> シグモイド関数と呼ばれる出力が0以上１以下となる関数である。
<br> 入力データの線形結合をシグモイド関数に入れることで出力データが目的変数(クラス)が１となる確率の値となる。
<br> 未知のパラメータは最尤推定法で算出する。
<br> 尤度関数にマイナスをつけ対数をとった関数を交差エントロピー誤差関数という。この関数を最小化する問題に置き換えられる。
<br> 最小化するパラメータを直接求めるのは難しいため、勾配降下法という逐次的に求める手法を利用します。
<br> 勾配降下法は総和を求める必要があるため、次元が大きな場合には計算時間がかかるなどの問題点がある。
<br> そのため、確率的勾配降下法（SGD）を用い、総和を求めるのではなく、データをランダムで選んでくる（データの一部分しか見ない）場合もある。
<br> 
<br> ④ステージテスト・黒本について
<br> ステージテストにおいて、以下が出題された。
<br> ・機械学習の種別について、ロジスティック回帰が”教師あり学習の分類手法”であること
<br> ・学習は”最尤推定法”によって行うこと
<br> ・シグモイド関数とは
<br> ・ロジスティック回帰の例
<br> 線形回帰と同じく、基本的なアルゴリズムがどんな学習か、どんな手法を使用して導出するかは押さえておきたい。
<br> ロジスティック回帰の分類手法については復習しておきたい。
<br> 高次の特徴空間へ写像したり、新たな特徴量にしてからロジスティック回帰を適用することで、完全分類可能である点は覚えておきたい。
<br> （黒本p.105〜111）
<br> ロジスティック回帰については、シグモイド関数、尤度関数やオッズに関する問題が出題されている。
<br> 計算だけでなく、実装に関する問題もあるので必ず復習しておきたい。
<br> 
<br> また、ステージテスト後半ではロジスティック回帰の導出にも利用する勾配降下法についての出題があった。
<br> ・勾配降下法と確率的勾配降下法の違い
<br> ・確率的勾配降下法のうちモメンタム法
<br> ・確率的勾配降下法とモメンタム法の実例
<br> 十分な理解はまだまだだが、計算して導出できるようにしておく。
<br> （黒本p.213〜218）
<br> 確率的勾配降下法の実装に関する出題がある。
<br> モメンタム法以外にもネステロフのモメンタム法もあるので、合わせて理解しておきたい。

## 主成分分析
①要点
<br> 教師なし学習の一つで、変数が多い場合に、情報を損なわないように次元圧縮する手法である主成分分析について学んだ。
<br> 説明変数と目的変数との関係性を見ていくことも重要であるが、説明変数が多くなると、関係性を理解するのも難しい。
<br> そのため、モデル構築時の前処理として次元を圧縮する主成分分析を行うことも多い。
<br> まず、データをある軸に射影させることを考える。この時射影後の分散が大きいほど情報が多いと捉える。
<br> 射影データの分散が最大となるような軸を探す（第１主成分）
<br> 第１主成分と直行する軸の中で、軸上に射影したデータの分散が最大となる軸を探す（第２主成分）
<br> 以下同様・・・・
<br> 具体的には、分散共分散行列の固有値を昇順に並べ、対応する固有ベクトルを第k主成分と呼ぶ。
<br> また、寄与率（全体のどれだけの情報を含むか）を以下で定める。
<br> 第i主成分の寄与率は、i番目の固有値を総分散（固有値の総和）で除したもの。
<br> 第i主成分までの累積寄与率は、１〜i番目までの固有値の和を総分散で除したもの。
<br> 
<br> ④ステージテスト・黒本について
<br> ステージテスト３において、主成分分析について以下の出題があった。
<br> ・主成分分析において、分散が大きい成分が重要な成分。各種成分は互いに直行するように選ばれる。
<br> ・主成分分析の実例についての出題。主成分は正規化することに注意。
<br> ・主成分分析は、非線形変換することはできない。
<br> ・主成分の数は、次元削減が目的なので主成分は少ない方が良い。
<br> （黒本p.130〜133）
<br> 分散共分散行列は半正定値行列なので、固有値はゼロ以上となり負の場合を考える必要はない点を押さえておきたい。
<br> 主成分分析の実装に関する問題があるので十分に理解しておきたい。

## サポートベクターマシン
①要点
<br> 教師あり学習の一つで、カテゴリを識別する境界線を引く手法をサポートベクターマシンという。
<br> 境界線の引き方は色々あるが、それぞれのグループの中で最も境界線に近い点(サポートベクター)との距離（マージン）が最大となるように線を引くのがサポートベクターマシン。
<br> 境界線で訓練データを分離できることを仮定した線形SV分類をハードマージンと呼ぶ。一般には、訓練データを分類できることは少ないため、
<br> 分類可能性を仮定しない場合にはソフトマージンと呼ぶ。
<br> また、線形分離だけでなくカーネルトリックと呼ばれる手法により非線形分離を行うことも可能である。
<br> 
<br> ④ステージテスト・黒本について
<br> ステージテスト３において、サポートベクターマシンについて以下の出題があった。
<br> ・SVMは教師あり学習。マージンが最大となるような境界を学習。マージン上にある点をサポートベクトルと呼ぶ。
<br> ・一次元空間（直線）におけるSVMの実例。（計算できるようにしておきたい。）
<br> ・SVMの性質。サポートベクトルより離れたデータは予測に影響しないことは理解しておく。（学習データのほとんどはサポートベクトルにならない。）
<br> ・ソフトマージンの正則化係数Cの性質。（黒本に掲載はないが、ソフトマージンを理解する上で必要である。）
<br> ・RBFカーネルの性質。（指数の理解があればすぐにわかる。）
<br> ・正規化線形カーネルについて。（正規化という言葉を考えると自然に判断できる）
<br> ・非線形分離を行うカーネルとして、rbf・多項式・シグモイドといった例がある。
<br> ・決定的な出力（決定的識別）をするモデルとして代表的なものはSVMである。一方、ロジスティック回帰は確率的な出力（確率的識別）である。
<br> （黒本p.112〜113）
<br> ・ソフトマージンの場合の誤識別に対するペナルティの表現をスラック変数により行い、
<br> 　マージンを最大化するとともにペナルティの最小化を行う。
<br> ・1クラスSVMは1クラスのデータを使って学習するSVMで、異常検知に使用される。
<br> ・LIBSVMは、SVMのライブラリ名称。
<br> ・カーネルトリックはSVM以外の線形回帰やロジスティック回帰などでも使用できる。
<br> ・代表例はガウスカーネル（動径基底関数、RBFカーネル）←講義資料の非線形回帰分析の基底にも実は出ていた。
